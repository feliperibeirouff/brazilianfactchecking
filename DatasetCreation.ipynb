{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess lupa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lupa\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import preprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "claims = []\n",
    "with open('lupa/bases/lupa_trechos_tratados.tsv','r', encoding='utf-8') as f:\n",
    "    read_claims = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "    i = 0\n",
    "    for claim in read_claims:\n",
    "        claim['claim_clean'] = preprocessing.tokenize_and_join(claim['claim'])\n",
    "        claim['evidence_clean'] = preprocessing.tokenize_and_join(claim['evidence'])\n",
    "        claims.append(claim)\n",
    "\n",
    "with Path('lupa/bases/lupa_trechos_limpos.tsv').open('w', encoding=\"utf-8\", newline='') as f2:\n",
    "    fieldnames = ['id', 'row', 'trecho', 'qtd_trechos', 'source', 'claim', 'claim_clean', 'metadata', 'class', 'evidence', 'evidence_clean']\n",
    "    print(fieldnames)\n",
    "    writer = csv.DictWriter(f2, fieldnames=fieldnames, delimiter='\\t', extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for trecho in claims:\n",
    "        #print(trecho['evidence'])\n",
    "        writer.writerow(trecho)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def standardizeDate(date):\n",
    "  date = date.replace(\"-\",\"/\")\n",
    "  try:\n",
    "    d = datetime.strptime(date, '%d/%m/%y')\n",
    "    date = d.strftime(\"%Y/%m/%d\")\n",
    "  except:\n",
    "    try:\n",
    "      d = datetime.strptime(date, '%d/%m/%Y')\n",
    "      date = d.strftime(\"%Y/%m/%d\")\n",
    "    except:\n",
    "      pass\n",
    "  return date\n",
    "\n",
    "\n",
    "def validItemFakeRecogna(item):\n",
    "  title = item['Titulo'].lower()\n",
    "  invalid_words = ['?', 'confira', 'entenda', 'veja', 'leia', 'últimas notícias', 'as notícias', ' y ', 'coisas que você precisa saber']  \n",
    "  invalid_words += ['casos de coronavírus no brasil']\n",
    "  for i in invalid_words:\n",
    "    if i in title:\n",
    "      return False\n",
    "  if title.split(' ')[0] == 'como':    \n",
    "    return False\n",
    "  return True\n",
    "\n",
    "claims = []\n",
    "global_row = 0\n",
    "\n",
    "with open('lupa/bases/lupa_trechos_limpos.tsv','r', encoding='utf-8') as f:\n",
    "    read_claims = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "    row = 0\n",
    "    for item in read_claims:\n",
    "      if item['class'] in ['VERDADEIRO', 'FALSO']:\n",
    "        claim = {}           \n",
    "        claim['id'] = global_row\n",
    "        claim['id_base'] = item['id']\n",
    "        claim['base'] = 'lupa'\n",
    "        claim['date_published']=item['source'].replace(\"https://piaui.folha.uol.com.br/lupa/\",\"\")[0:10]\n",
    "        claim['domain'] = 'piaui.folha.uol.com.br/lupa'\n",
    "        claim['document'] = item['source']\n",
    "        claim['class'] = item['class']\n",
    "        claim['claim'] = item['claim']\n",
    "        claim['evidence'] = item['evidence']\n",
    "        claims.append(claim)        \n",
    "        global_row +=1      \n",
    "      row +=1  \n",
    "\n",
    "with open('bases/FACTCKBR.tsv','r', encoding='utf-8') as f:\n",
    "  read = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "  row = 0\n",
    "  for item in read:\n",
    "    itemClass = item['alternativeName'].upper() \n",
    "    if (itemClass == 'FALSO' or itemClass == 'VERDADEIRO') and item['reviewBody'] and item['reviewBody'] !='Empty':   \n",
    "      claim = {}   \n",
    "      claim['id'] = global_row\n",
    "      claim['id_base'] = row\n",
    "      claim['base'] = 'factckbr'\n",
    "      claim['date_published']=item['datePublished']\n",
    "      claim['document'] = item['URL']\n",
    "      claim['domain'] = item['URL'].replace(\"http://\", \"\").replace(\"https://\", \"\").split(\"/\")[0]\n",
    "      claim['class'] = itemClass\n",
    "      claim['claim'] = item['claimReviewed']\n",
    "      claim['evidence'] = item['reviewBody']\n",
    "      claims.append(claim)      \n",
    "      global_row +=1\n",
    "    row +=1\n",
    "\n",
    "with open('bases/fakepedia_boatos.tsv','r', encoding='utf-8') as f:\n",
    "  read = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "  row = 0\n",
    "  for item in read:\n",
    "    if item['status'] == 'OK':      \n",
    "      claim = {}   \n",
    "      claim['id'] = global_row\n",
    "      claim['id_base'] = row\n",
    "      claim['base'] = 'fakepedia'\n",
    "      claim['date_published']=item['published']\n",
    "      claim['document'] = item['url']\n",
    "      claim['domain'] = item['url'].replace(\"http://\", \"\").replace(\"https://\", \"\").split(\"/\")[0]\n",
    "      claim['class'] = 'FALSO'\n",
    "      claim['claim'] = item['original_news']\n",
    "      claim['evidence'] = item['content']\n",
    "\n",
    "      claim['claim'] = re.sub(r'Confira dica para não cair, nunca mais, em boatos.*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'Ps.: Esse artigo é uma sugestão .*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'PS: esse artigo foi uma sugestão .*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'S: esse artigo foi uma sugestão .*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'Este texto foi escrito pelo leitor .*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'P.S.: Esse artigo é uma sugestão de leitores .*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'Ps.: Esse artigo foi uma sugestão .*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'Os 10 boatos que mais bombaram.*',\"\", claim['claim'], flags=re.S)\n",
    "\n",
    "      claim['claim'] = re.sub(r'<span .*</span>',\"\", claim['claim'], flags=re.S)\n",
    "      claim['evidence'] = re.sub(r'<span .*</span>',\"\", claim['evidence'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'Lista de fake news das eleições .*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'Confira a lista de todas as fake news .*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'Clique nos links “bit.ly” para acessar nossos perfis:.*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = re.sub(r'– Siga-nos no Facebook.*',\"\", claim['claim'], flags=re.S)\n",
    "      claim['claim'] = claim['claim'].strip()\n",
    "      \n",
    "      if claim['claim']:\n",
    "        claims.append(claim)\n",
    "        global_row +=1\n",
    "    row +=1\n",
    "\n",
    "with open('bases/fakeRecogna.tsv','r', encoding='utf-8') as f:\n",
    "  read = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "  row = 0  \n",
    "  for item in read:    \n",
    "    if item['Classe'] == '1' and validItemFakeRecogna(item):   \n",
    "      claim = {}   \n",
    "      claim['id'] = global_row\n",
    "      claim['id_base'] = row\n",
    "      claim['base'] = 'fakeRecogna'\n",
    "      claim['date_published']=item['Data']\n",
    "      claim['document'] = item['URL']\n",
    "      claim['domain'] = item['URL'].replace(\"http://\", \"\").replace(\"https://\", \"\").split(\"/\")[0]\n",
    "      claim['class'] = 'VERDADEIRO'\n",
    "      claim['claim'] = item['Titulo']\n",
    "      claim['evidence'] = item['Noticia']\n",
    "      claims.append(claim)      \n",
    "      global_row +=1\n",
    "    row +=1\n",
    "\n",
    "bad_documents = []\n",
    "#Put in this file urls that are not good to include in the dataset\n",
    "with open('documentos_ruins.txt','r', encoding='utf-8') as f:\n",
    "  read = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "  for item in read:\n",
    "    bad_documents.append(item['url'])\n",
    "\n",
    "bad_claims = []\n",
    "#Put in this file the text of claims that are not good to include in the dataset\n",
    "with open('claims_ruins.txt','r', encoding='utf-8') as f:\n",
    "  read = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "  for item in read:\n",
    "    bad_claims.append(item['claim'])\n",
    "\n",
    "claims = [claim for claim in claims if claim['document'] not in bad_documents and claim['claim'] not in bad_claims]\n",
    "\n",
    "#Removing duplicates\n",
    "claims_without_duplicates = []\n",
    "for i in range(len(claims)):\n",
    "  duplicated = False\n",
    "  for c in claims_without_duplicates:\n",
    "    if claims[i]['claim'] == c['claim']:\n",
    "      duplicated = True\n",
    "      break\n",
    "  if not duplicated:\n",
    "    claims_without_duplicates.append(claims[i])\n",
    "claims = claims_without_duplicates\n",
    "\n",
    "for claim in claims:\n",
    "  claim['num_words_claim'] = len(claim['claim'].split(' '))\n",
    "  claim['num_words_evidence'] = len(claim['evidence'].split(' '))\n",
    "  claim['num_chars_claim'] = len(claim['claim'])\n",
    "  claim['num_chars_evidence'] = len(claim['evidence'])\n",
    "  claim['date_published'] = claim['date_published'].replace(\"Publicado em  \", \"\").split(\" \")[0]\n",
    "\n",
    "#limit 300 characters\n",
    "claims = [claim for claim in claims if claim['num_chars_claim'] <= 300]\n",
    "\n",
    "qtd_fake = 0\n",
    "qtd_true = 0\n",
    "for claim in claims:\n",
    "  if claim['class'] == 'FALSO':\n",
    "    qtd_fake+=1\n",
    "  if claim['class'] == 'VERDADEIRO':\n",
    "    qtd_true+=1\n",
    "\n",
    "#removes excess of true claims\n",
    "diferenca = qtd_true - qtd_fake\n",
    "removed = 0\n",
    "for i in range(len(claims) - 1, -1, -1):\n",
    "  if removed >= diferenca:\n",
    "    break\n",
    "  if claims[i]['class'] == 'VERDADEIRO':\n",
    "    del claims[i]\n",
    "    removed += 1\n",
    "print(qtd_fake, qtd_true, 'removed' , removed)\n",
    "\n",
    "regex = datetime.strptime\n",
    "\n",
    "#Standardize dates\n",
    "for c in claims:\n",
    "  c['date_published'] = c['date_published'].replace(\"-\",\"/\")\n",
    "  try:\n",
    "    d = regex(c['date_published'], '%d/%m/%y')\n",
    "    c['date_published'] = d.strftime(\"%Y/%m/%d\")\n",
    "  except:\n",
    "    try:\n",
    "      d = regex(c['date_published'], '%d/%m/%Y')\n",
    "      c['date_published'] = d.strftime(\"%Y/%m/%d\")\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "with Path('bases/base3/base3_raw.tsv').open('w', encoding=\"utf-8\", newline='') as f2:\n",
    "    fieldnames = ['id', 'base', 'claim', 'evidence','document', 'date_published', 'domain', 'class', 'num_words_claim', 'num_words_evidence', 'num_chars_claim', 'num_chars_evidence']\n",
    "    print(fieldnames)\n",
    "    writer = csv.DictWriter(f2, fieldnames=fieldnames, delimiter='\\t', extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    row = 0\n",
    "    for row, claim in enumerate(claims):\n",
    "        claim['id'] = row\n",
    "        writer.writerow(claim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import preprocessing\n",
    "\n",
    "def containsText(t, ignore_sentences_containing):\n",
    "    for sentence in ignore_sentences_containing:\n",
    "        if sentence in t:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_claim(text):\n",
    "    text = text.replace('Versão 1:', '')\n",
    "    text = text.replace('Versão 2:', '')\n",
    "    text = text.replace('(sic)', '')\n",
    "    tokens = preprocessing.tokenize(text)\n",
    "    ignore_sentences_containing = ['Se você quiser sugerir um tema para o Boatos.org']\n",
    "    ignore_exact_sentences = ['Ps.', 'Leia a mensagem que circula online:', '']\n",
    "\n",
    "    tokens = [t for t in tokens if t and t not in ignore_exact_sentences ]\n",
    "    tokens = [t for t in tokens if not containsText(t, ignore_sentences_containing)  ]\n",
    "    return '\\n'.join(tokens)\n",
    "\n",
    "def clean_evidence(text):\n",
    "    text = text.replace('_NOTICIA_ORIGINAL_', '')\n",
    "\n",
    "    \n",
    "    tokens = preprocessing.tokenize(text)\n",
    "    ignore_sentences_containing = ['Esse artigo é uma sugestão de leitores do Boatos.org', 'Jornalista e caçador de falcatruas na internet', \n",
    "                           'Se você quiser sugerir um tema', 'Esse artigo é uma sugestão de diversos leitores', 'Clique nos links \"bit.ly\"', ]\n",
    "    ignore_exact_sentences = ['Ps.', 'Leia a mensagem que circula online:', '']\n",
    "\n",
    "    tokens = [t for t in tokens if t and t not in ignore_exact_sentences ]\n",
    "    tokens = [t for t in tokens if not containsText(t, ignore_sentences_containing)  ]\n",
    "    return '\\n'.join(tokens)\n",
    "\n",
    "claims = []\n",
    "with Path('bases/base3/base3_raw.tsv').open('r', encoding=\"utf-8\", newline='') as f:\n",
    "    read = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "    for item in read:\n",
    "        item['claim_clean'] = clean_claim(item['claim'])\n",
    "        item['evidence_clean'] = clean_evidence(item['evidence'])\n",
    "        claims.append(item)\n",
    "\n",
    "with Path('bases/base3/base3.tsv').open('w', encoding=\"utf-8\", newline='') as f2:\n",
    "    fieldnames = ['id', 'base', 'claim', 'evidence', 'claim_clean', 'evidence_clean', 'document', 'date_published', 'domain', 'class', 'num_words_claim', 'num_words_evidence', 'num_chars_claim', 'num_chars_evidence']\n",
    "    writer = csv.DictWriter(f2, fieldnames=fieldnames, delimiter='\\t', extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for claim in claims:\n",
    "        writer.writerow(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def readClaims(basepath):\n",
    "  documents = set()\n",
    "  claims = []\n",
    "  with open(basepath,'r', encoding='utf-8') as f:\n",
    "    read = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "    for item in read:\n",
    "      item['id'] = int(item['id'])      \n",
    "      claims.append(item)\n",
    "      documents.add(item['document'])\n",
    "    documents = list(documents)\n",
    "    return claims, documents\n",
    "\n",
    "def writeClaims(basepath, claims):\n",
    "  with Path(basepath).open('w', encoding=\"utf-8\", newline='') as f2:\n",
    "    #fieldnames = ['id', 'base', 'id_base', 'claim_clean', 'evidence_clean','document','class']\n",
    "    fieldnames = ['id', 'base', 'claim', 'evidence', 'claim_clean', 'evidence_clean', 'document', 'date_published', 'domain', 'class', 'num_words_claim', 'num_words_evidence', 'num_chars_claim', 'num_chars_evidence']\n",
    "    print(fieldnames)\n",
    "    writer = csv.DictWriter(f2, fieldnames=fieldnames, delimiter='\\t', extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for trecho in claims:\n",
    "        #print(trecho['evidence'])\n",
    "        writer.writerow(trecho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims, documents = readClaims('bases/base3/base3.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset into training, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'base', 'claim', 'evidence', 'claim_clean', 'evidence_clean', 'document', 'date_published', 'domain', 'class', 'num_words_claim', 'num_words_evidence', 'num_chars_claim', 'num_chars_evidence']\n",
      "['id', 'base', 'claim', 'evidence', 'claim_clean', 'evidence_clean', 'document', 'date_published', 'domain', 'class', 'num_words_claim', 'num_words_evidence', 'num_chars_claim', 'num_chars_evidence']\n",
      "['id', 'base', 'claim', 'evidence', 'claim_clean', 'evidence_clean', 'document', 'date_published', 'domain', 'class', 'num_words_claim', 'num_words_evidence', 'num_chars_claim', 'num_chars_evidence']\n"
     ]
    }
   ],
   "source": [
    "#Divide dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "documents_train, documents_test = train_test_split(documents, test_size=0.2, random_state=1)\n",
    "documents_train, documents_valid = train_test_split(documents_train, test_size=0.15, random_state=3)\n",
    "claims_train = []\n",
    "claims_valid = []\n",
    "claims_test = []\n",
    "for c in claims:\n",
    "  if c['document'] in documents_train:\n",
    "    claims_train.append(c)\n",
    "  elif c['document'] in documents_test:\n",
    "    claims_test.append(c)\n",
    "  elif c['document'] in documents_valid:\n",
    "    claims_valid.append(c)\n",
    "\n",
    "writeClaims('bases/base3/base3_train.tsv', claims_train)\n",
    "\n",
    "writeClaims('bases/base3/base3_valid.tsv', claims_valid)\n",
    "\n",
    "writeClaims('bases/base3/base3_test.tsv', claims_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_train, doc_train = readClaims('bases/base3/base3_train.tsv')\n",
    "claims_valid, doc_valid = readClaims('bases/base3/base3_valid.tsv')\n",
    "claims_test, doc_test = readClaims('bases/base3/base3_test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate negative samples (samples with class 'Not enough information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "modelsbert = SentenceTransformer('models/portuguese_sentence_transformer')\n",
    "device = 'cuda'\n",
    "modelsbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.spatial\n",
    "\n",
    "import torch\n",
    "#Esse aqui fez com a média: https://medium.com/analytics-vidhya/few-shot-learning-using-sbert-95f8b08248bf\n",
    "def sbert_sentence_embedding(claim, model = modelsbert):\n",
    "    sentences = claim.split(\"\\n\")\n",
    "    encoded = model.encode(sentences)\n",
    "    return torch.mean(torch.Tensor(encoded), dim=0)\n",
    "\n",
    "def sbert_sentence_embedding_total(claim, model = modelsbert):\n",
    "    sentences = claim.split(\"\\n\")\n",
    "    encoded = model.encode(sentences)\n",
    "    return encoded\n",
    "\n",
    "def cos_similarity(sent1_emb, sent2_emb):\n",
    "  return 1 - scipy.spatial.distance.cosine(sent1_emb, sent2_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSbertEmbedding(samples):\n",
    "    i = 0\n",
    "    print(len(samples))\n",
    "    for claim in samples:\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        i += 1        \n",
    "        claim['claim_sbert'] = sbert_sentence_embedding(claim['claim_clean'])\n",
    "        claim['evidence_sbert'] = sbert_sentence_embedding(claim['evidence_clean'])\n",
    "\n",
    "def sortFunction(e):\n",
    "  return -e['value']\n",
    "\n",
    "def getSimilarities(claim_sbert, evidences_sbert):\n",
    "    similarities = []\n",
    "    for i, evidence_sbert in enumerate(evidences_sbert):\n",
    "        similarity = cos_similarity(evidence_sbert, claim_sbert)\n",
    "        similarities.append({'row': i, 'value': similarity})\n",
    "    similarities.sort(key=sortFunction)\n",
    "    return similarities\n",
    "\n",
    "def generateNegative(positive_samples):        \n",
    "    negative_samples = []\n",
    "    claim_row = 0\n",
    "    for claim in positive_samples:\n",
    "        if claim_row % 100 == 0:\n",
    "            print(claim_row)\n",
    "        claim_row += 1\n",
    "        #new fields\n",
    "        claim['id_claim'] = claim['id']\n",
    "        claim['id_evidence'] = claim['id']\n",
    "        claim['document_evidence'] = claim['document']\n",
    "\n",
    "        #Find most similar evidences\n",
    "        evidences_sbert = [evidence['evidence_sbert'] for evidence in positive_samples]\n",
    "        claim_sbert = claim['claim_sbert']\n",
    "        similarities = getSimilarities(claim_sbert, evidences_sbert)\n",
    "\n",
    "        #Take 5 evidences, from documents different from the true evidence, and with high similarity to the claim.\n",
    "        count = 1\n",
    "        i = 0\n",
    "        while count <= 5:\n",
    "            evidenceRow = similarities[i]['row']\n",
    "            evidence = positive_samples[evidenceRow]\n",
    "            if evidence['document'] != claim['document']:\n",
    "                claimNegative = claim.copy()\n",
    "                claimNegative['class'] = 'INSUFICIENTE'            \n",
    "                claimNegative['id'] = -(claim['id']*10 + count)\n",
    "                claimNegative['id_claim'] = claim['id']\n",
    "                claimNegative['id_evidence'] = evidence['id']\n",
    "                claimNegative['claim_clean'] = claim['claim_clean']\n",
    "                claimNegative['evidence_clean'] = evidence['evidence_clean']            \n",
    "                claimNegative['document_evidence'] = evidence['document']\n",
    "                negative_samples.append(claimNegative)\n",
    "                count+=1\n",
    "            i += 1\n",
    "    negative_samples.extend(positive_samples)\n",
    "    return negative_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateSbertEmbedding(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in claims_test:\n",
    "  c['claim_sbert'] = claims[c['id']]['claim_sbert']\n",
    "  c['evidence_sbert'] = claims[c['id']]['evidence_sbert']\n",
    "\n",
    "\n",
    "for c in claims_train:\n",
    "  c['claim_sbert'] = claims[c['id']]['claim_sbert']\n",
    "  c['evidence_sbert'] = claims[c['id']]['evidence_sbert']\n",
    "\n",
    "for c in claims_valid:\n",
    "  c['claim_sbert'] = claims[c['id']]['claim_sbert']\n",
    "  c['evidence_sbert'] = claims[c['id']]['evidence_sbert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_valid3classes = generateNegative(claims_valid)\n",
    "claim_test3classes = generateNegative(claims_test)\n",
    "claim_train3classes = generateNegative(claims_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeClaims3classes(basepath, claims):\n",
    "  with Path(basepath).open('w', encoding=\"utf-8\", newline='') as f2:\n",
    "    fieldnames = ['id', 'base', 'id_claim', 'id_evidence', 'claim_clean', 'evidence_clean','document', 'document_evidence','class']\n",
    "    print(fieldnames)\n",
    "    writer = csv.DictWriter(f2, fieldnames=fieldnames, delimiter='\\t', extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for trecho in claims:\n",
    "        #print(trecho['evidence'])\n",
    "        writer.writerow(trecho)\n",
    "\n",
    "writeClaims3classes('bases/base3/base3_valid_3classes.tsv', claim_valid3classes)\n",
    "writeClaims3classes('bases/base3/base3_train_3classes.tsv', claim_train3classes)\n",
    "writeClaims3classes('bases/base3/base3_test_3classes.tsv', claim_test3classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a5609854b1d1af0719aa43b8184aabedc7c5a84215f90de6c4b3f051bd446ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
