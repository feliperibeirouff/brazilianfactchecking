{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_loader\n",
    "import re\n",
    "\n",
    "def normalize_url(url):\n",
    "   url = url.strip()\n",
    "   url = re.sub('/$', '', url)\n",
    "   url = url.replace(\"lupa.uol.com.br/jornalismo\", \"piaui.folha.uol.com.br/lupa\")\n",
    "   return url\n",
    "\n",
    "base_path = \"bases/base3/\"\n",
    "claims_file = \"base3_test.tsv\"\n",
    "\n",
    "claims = dataset_loader.loadClaims(base_path + claims_file)\n",
    "for claim in claims:\n",
    "  claim['document'] = normalize_url(claim['document'])\n",
    "\n",
    "def search_claim(claim_id):\n",
    "   for claim in claims:\n",
    "      if claim['id'] == claim_id:\n",
    "         return claim\n",
    "   print(claim_id, 'not found')\n",
    "   return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strategies import ClassifierStrategy, EvidenceSelectStrategy\n",
    "import csv\n",
    "from collections import defaultdict   \n",
    "\n",
    "class FactCheckerParameters:\n",
    "    def __init__(self, document_search_method, evidence_selection_method, classifier_method, base_path, url_categories_path, claims_file):\n",
    "        self.document_search_method = document_search_method\n",
    "        self.evidence_selection_method = evidence_selection_method\n",
    "        self.classifier_method = classifier_method\n",
    "        self.base_path = base_path\n",
    "        self.url_categories_path = url_categories_path\n",
    "        self.claims_path = base_path  + claims_file\n",
    "        self.claims_unprocessed_path = self.claims_path.replace(\".tsv\", \"_raw.tsv\")\n",
    "        self.claims_sbert_path = self.claims_path.replace(\".tsv\", \"_sbert1.pt\")\n",
    "        self.evidences_path = self.claims_path.replace(\".tsv\", \"_ev\" + self.evInfo() + \".tsv\")\n",
    "        self.evidences_classified_path = self.evidences_path.replace('.tsv',  \"_\" + classifier_method.name + '_classified.tsv')\n",
    "        self.claims_classified_path = self.claims_path.replace('.tsv', self.classifierInfo() + '_classified.tsv')\n",
    "        self.documents_path = self.claims_path.replace(\".tsv\", self.docInfo() + \"_docret.tsv\")\n",
    "        self.urls_directory = self.getUrlsDir(self.documents_path)\n",
    "        self.claims_classified_same_doc_path = self.claims_path.replace('.tsv', self.classifierInfo() + '_classifiedSameDoc.tsv')\n",
    "        \n",
    "        #print(\"base_path:\", self.base_path)\n",
    "        #print(\"url_categories_path:\", self.url_categories_path)\n",
    "        #print(\"claims_path:\", self.claims_path)\n",
    "        #print(\"claims_unprocessed_path:\", self.claims_unprocessed_path)\n",
    "        #print(\"claims_sbert_path:\", self.claims_sbert_path)\n",
    "        #print(\"evidences_path:\", self.evidences_path)\n",
    "        #print(\"evidences_classified_path:\", self.evidences_classified_path)\n",
    "        #print(\"claims_classified_path:\", self.claims_classified_path)\n",
    "        #print(\"documents_path:\", self.documents_path)\n",
    "        #print(\"urls_directory:\", self.urls_directory)\n",
    "\n",
    "    def getUrlsDir(self, doc_path):\n",
    "        return doc_path.replace(\".tsv\", \"_urls/\")\n",
    "\n",
    "    def docInfo(self):\n",
    "        return \"_\" + self.document_search_method\n",
    "\n",
    "    def evInfo(self):\n",
    "        return self.docInfo() + \"_\" + self.evidence_selection_method.name\n",
    "\n",
    "    def classifierInfo(self):\n",
    "        return self.evInfo() + \"_\" + self.classifier_method.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION OF THE WHOLE FACT-CHECKING PROCESS WITH DIFFERENT STRATEGIES FOR SELECTING THE EVIDENCE\n",
    "\n",
    "evStrategies = [\n",
    "  EvidenceSelectStrategy.Sentence1,\n",
    "  EvidenceSelectStrategy.Sentence1NoQuotes,\n",
    "  EvidenceSelectStrategy.TitleSentence1,\n",
    "  EvidenceSelectStrategy.Context5,\n",
    "  EvidenceSelectStrategy.Context5NoQuotes,\n",
    "  EvidenceSelectStrategy.TitleContext5,\n",
    "  EvidenceSelectStrategy.TitleContext5NoQuotes,\n",
    "  EvidenceSelectStrategy.OnlyTitle,\n",
    "]\n",
    "\n",
    "classStrategies = [\n",
    "  ClassifierStrategy.Bert3  \n",
    "]\n",
    "docStrategies = [\"acurrent\"]\n",
    "\n",
    "\n",
    "print('method', 'true_supports','true_refutes', 'true_NEI', 'false_supports','false_refutes', 'false_NEI', 'ptrue_supports', 'ptrue_refutes', 'ptrue_NEI', 'pfalse_supports', 'pfalse_refutes', 'pfalse_NEI', 'precision_refutes', 'recall_false', 'f1_refutes', 'precision_supports', 'recall_true', 'f1_supports', 'accuracy', 'avg_precision', 'avg_recall', 'avg_f1', sep=\"\\t\")\n",
    "for docStrategy in docStrategies:\n",
    "  for classStrategy in classStrategies:\n",
    "    for evStrategy in evStrategies:\n",
    "      params = FactCheckerParameters(\n",
    "          document_search_method = docStrategy,\n",
    "          evidence_selection_method = evStrategy,\n",
    "          classifier_method = classStrategy,\n",
    "          base_path = \"bases/base3/\",\n",
    "          url_categories_path = \"urls_categories.txt\",\n",
    "          claims_file = \"base3_test.tsv\",\n",
    "      )\n",
    "      samples = []\n",
    "      dict_count = defaultdict(int)      \n",
    "      path = params.claims_classified_path\n",
    "      #path = params.evidences_classified_path\n",
    "      try:\n",
    "        with open(path,'r', encoding='utf-8') as f:\n",
    "          read_samples = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "          i = 0\n",
    "          total = 0\n",
    "          for sample in read_samples:             \n",
    "            sample['claim_id'] = int(sample['claim_id'])\n",
    "            dict_count[(sample['claim_class'], sample['predicted_label'])] += 1\n",
    "            total += 1\n",
    "            samples.append(sample)\n",
    "\n",
    "        #for key in dict_count.keys():\n",
    "        #  print(params.claims_classified_path, key[0], key[1], dict_count[key], sep='\\t')\n",
    "        true_supports = dict_count[('VERDADEIRO', 'SUPORTA')]\n",
    "        true_refutes = dict_count[('VERDADEIRO', 'REFUTA')]\n",
    "        true_NEI = dict_count[('VERDADEIRO', 'INSUFICIENTE')]       \n",
    "        false_supports = dict_count[('FALSO', 'SUPORTA')] \n",
    "        false_refutes = dict_count[('FALSO', 'REFUTA')] \n",
    "        false_NEI = dict_count[('FALSO', 'INSUFICIENTE')] \n",
    "        accuracy = (true_supports + false_refutes)/total\n",
    "        recall_true = true_supports/(true_supports + true_refutes + true_NEI)\n",
    "        recall_false = false_refutes/(false_supports + false_refutes + false_NEI)\n",
    "        precision_supports = true_supports/(true_supports + false_supports)\n",
    "        precision_refutes = false_refutes/(true_refutes + false_refutes)\n",
    "        f1_supports = 2/(precision_supports**(-1)+recall_true**(-1))\n",
    "        f1_refutes = 2/(precision_refutes**(-1)+recall_false**(-1))\n",
    "        avg_precision = (precision_supports + precision_refutes)/2\n",
    "        avg_recall = (recall_true + recall_false)/2\n",
    "        avg_f1 = (f1_supports + f1_refutes)/2\n",
    "        ptrue_supports = true_supports/float(true_supports + true_refutes + true_NEI)\n",
    "        ptrue_refutes = true_refutes/float(true_supports + true_refutes + true_NEI)\n",
    "        ptrue_NEI = true_NEI/float(true_supports + true_refutes + true_NEI)\n",
    "        pfalse_supports = false_supports/float(false_supports + false_refutes + false_NEI)\n",
    "        pfalse_refutes = false_refutes/float(false_supports + false_refutes + false_NEI)\n",
    "        pfalse_NEI = false_NEI/float(false_supports + false_refutes + false_NEI)\n",
    "\n",
    "        print(path.replace(params.base_path, \"\"), true_supports,true_refutes, true_NEI, false_supports,false_refutes, false_NEI, ptrue_supports,ptrue_refutes, ptrue_NEI, pfalse_supports,pfalse_refutes, pfalse_NEI, precision_refutes, recall_false, f1_refutes, precision_supports, recall_true, f1_supports, accuracy, avg_precision, avg_recall, avg_f1, sep=\"\\t\")\n",
    "      except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION OF PERCENTAGE OF CLAIMS CORRECTLY CLASSIFIED WITH AT LEAST ONE PIECE OF EVIDENCE CORRECT FROM THE ANNOTATED DOCUMENT\n",
    "\n",
    "import csv\n",
    "\n",
    "def readDictFile(filepath, delimiter = '\\t'):\n",
    "  samples = []\n",
    "  with open(filepath,'r', encoding='utf-8') as f:\n",
    "    read_samples = csv.DictReader(f, delimiter=delimiter, skipinitialspace=True)\n",
    "    for sample in read_samples:\n",
    "      samples.append(sample)\n",
    "  return samples\n",
    "\n",
    "\n",
    "\n",
    "claims = []\n",
    "\n",
    "for classStrategy in classStrategies:\n",
    "  for evStrategy in evStrategies:\n",
    "    params = FactCheckerParameters(\n",
    "        document_search_method = \"acurrent\",\n",
    "        evidence_selection_method = evStrategy,\n",
    "        classifier_method = classStrategy,\n",
    "        base_path = \"bases/base3/\",\n",
    "        url_categories_path = \"urls_categories.txt\",\n",
    "        claims_file = \"base3_test.tsv\",\n",
    "    )\n",
    "    evidences_classified = readDictFile(params.evidences_classified_path)\n",
    "    claims_classified = readDictFile(params.claims_classified_path)\n",
    "\n",
    "    for e in evidences_classified:\n",
    "      e['url'] = normalize_url(e['url'])\n",
    "\n",
    "    if claims == []:\n",
    "      claims = dataset_loader.loadClaims(params.claims_path)\n",
    "      for c in claims:\n",
    "        c['document'] = normalize_url(c['document'])\n",
    "      \n",
    "    count_true = 0\n",
    "    count_false = 0\n",
    "    count_same_document = 0\n",
    "    count_same_document_true = 0\n",
    "    count_same_document_false = 0\n",
    "\n",
    "    \n",
    "    with open(params.claims_classified_same_doc_path, 'w',  encoding=\"utf-8\", newline='') as f:\n",
    "      fieldnames = ['claim_id', 'claim_class', 'predicted_label', 'correct', 'correct_same_document', 'document']\n",
    "      writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='\\t', extrasaction='ignore')\n",
    "      writer.writeheader()\n",
    "      \n",
    "      for c in claims_classified:\n",
    "        correct = 0\n",
    "        same_document = 0\n",
    "        claim_document = ''\n",
    "        for claim in claims:          \n",
    "          if claim['id'] == int(c['claim_id']):\n",
    "            claim_document = claim['document']\n",
    "            break\n",
    "        if c['claim_class'] == 'FALSO':\n",
    "          count_true += 1\n",
    "        else:\n",
    "          count_false += 1\n",
    "        if (c['claim_class'] == 'FALSO' and c['predicted_label'] == 'REFUTA') or (c['claim_class'] == 'VERDADEIRO' and c['predicted_label'] == 'SUPORTA'):\n",
    "          correct = 1            \n",
    "\n",
    "          for e in evidences_classified:\n",
    "            if e['claim_id'] == c['claim_id'] and e['predicted_label'] == c['predicted_label']:\n",
    "              if e['url'] == claim_document:\n",
    "                same_document = 1\n",
    "          if same_document:\n",
    "            count_same_document += 1\n",
    "            if c['claim_class'] == 'FALSO':\n",
    "              count_same_document_false += 1\n",
    "            else:\n",
    "              count_same_document_true += 1\n",
    "        writer.writerow({\n",
    "          'claim_id': c['claim_id'], \n",
    "          'claim_class': c['claim_class'], \n",
    "          'predicted_label': c['predicted_label'],\n",
    "          'correct': correct,\n",
    "          'correct_same_document': same_document,\n",
    "          'document' : claim_document\n",
    "          })\n",
    "      print('***',evStrategy.name, len(claims_classified), count_same_document, count_true, count_same_document_true,count_false,count_same_document_false, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
