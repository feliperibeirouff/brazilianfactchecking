{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import csv\n",
    "\n",
    "claims = []\n",
    "\n",
    "with open('bases/allLupaFiles.tsv', 'r', encoding='utf-8') as f:\n",
    "    read_claims = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "    for claim in read_claims:\n",
    "        claims.append(claim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def endsWithQuotes(text):\n",
    "    return text.endswith(\"″\") or text.endswith(\"”\") or text.endswith(\"“\") or text.endswith(\"”.\")\n",
    "\n",
    "def starsWithQuotes(text):\n",
    "    return text.startswith(\"“\") or text.startswith(\"”\")\n",
    "\n",
    "def getParagraph(texts, initPos, endPos):\n",
    "    paragraph = texts[initPos]\n",
    "    for pos in range(initPos + 1, endPos + 1):\n",
    "        paragraph += \"\\n\" + texts[pos]\n",
    "    return paragraph\n",
    "\n",
    "\n",
    "def preprocessLupa(text):\n",
    "    classes = ['VERDADEIRO, MAS', 'VERDADEIRO', 'EXAGERADO', 'FALSO', 'AINDA É CEDO PARA DIZER', 'CONTRADITÓRIO', 'SUBESTIMADO', 'INSUSTENTÁVEL', 'DE OLHO']    \n",
    "    texts = text.split(\"\\n\")\n",
    "    parts_to_ignore = ['Nota:', 'Assine a Lente, a newsletter gratuita sobre desinformação da Lupa!',\n",
    "        'O conteúdo produzido pela Lupa', 'Editado por:', 'Essa informação também foi verificada', 'Esta verificação foi sugerida por leitores', \n",
    "        'Uma versão semelhante dessa checagem foi feita', 'Nota da redação:']\n",
    "\n",
    "    texts_treated = []\n",
    "    for text in texts:\n",
    "        ignore = False\n",
    "        for part in parts_to_ignore:\n",
    "            if part in text:\n",
    "                ignore = True\n",
    "                break\n",
    "        if not ignore and text.strip() != '':\n",
    "            texts_treated.append(text.strip())\n",
    "    texts = texts_treated\n",
    "\n",
    "    items = []\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        for classe in classes:\n",
    "            if texts[i] == classe:                \n",
    "                item = {'class': classe, 'metadata':'', 'classe_pos': i, 'trecho': len(items)}\n",
    "\n",
    "                if endsWithQuotes(texts[i-1]):\n",
    "                    endPos = i - 1\n",
    "                    initPos = endPos\n",
    "                    while not starsWithQuotes(texts[initPos]):\n",
    "                        initPos -= 1                    \n",
    "                    item['claim'] = getParagraph(texts, initPos, endPos)                    \n",
    "                    item['claim_pos'] = initPos\n",
    "                elif starsWithQuotes(texts[i-2]):\n",
    "                    item['claim'] = texts[i-2]\n",
    "                    item['claim_pos'] = i-2\n",
    "                    item['metadata'] = texts[i-1]\n",
    "                elif endsWithQuotes(texts[i-2]):\n",
    "                    endPos = i - 2\n",
    "                    initPos = endPos\n",
    "                    while not starsWithQuotes(texts[initPos]):\n",
    "                        initPos -= 1\n",
    "                    item['claim'] = getParagraph(texts, initPos, endPos)                    \n",
    "                    item['claim_pos'] = initPos\n",
    "                    item['metadata'] = texts[i-1]\n",
    "                elif texts[i-1].startswith('Checagem original') or texts[i-1].startswith('Texto em post'):\n",
    "                    item['claim'] = texts[i-2]\n",
    "                    item['claim_pos'] = i-2\n",
    "                    item['metadata'] = texts[i-1]\n",
    "                else:\n",
    "                    item['claim'] = texts[i-1]\n",
    "                    item['claim_pos'] = i-1\n",
    "                item['claim'] = re.sub(r'^\\d+. (.+)',r'\\1', item['claim'])\n",
    "                item['evidence'] = ''\n",
    "\n",
    "                if len(items) > 0:\n",
    "                    previousItem = items[-1]\n",
    "                    initPosEvidence = previousItem['classe_pos'] + 1\n",
    "                    endPosEvidence = item['claim_pos'] - 1\n",
    "                    previousItem['evidence'] = getParagraph(texts, initPosEvidence, endPosEvidence)\n",
    "                items.append(item)\n",
    "                break\n",
    "    for item in items:\n",
    "        item['qtd_trechos'] = len(items)\n",
    "\n",
    "    lastItem = items[-1]\n",
    "    initPosEvidence = lastItem['classe_pos'] + 1\n",
    "    endPosEvidence = len(texts) - 1\n",
    "    lastItem['evidence'] = getParagraph(texts, initPosEvidence, endPosEvidence)\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "items = []\n",
    "\n",
    "count = 0\n",
    "for i, claim in enumerate(claims):\n",
    "    items_claim = preprocessLupa(claim['content'])\n",
    "    for item in items_claim:\n",
    "        \n",
    "        item['source'] = claim['url']\n",
    "        item['row'] = claim['row']\n",
    "        item['id'] = count\n",
    "        if i == 433:\n",
    "            print(item)\n",
    "            print(len(items))\n",
    "        items.append(item)            \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\"Em julho de 2015, o Irã e um grupo de seis países nominado P5+1 (Estados Unidos, China, França, Rússia, Reino Unido e Alemanha), junto com a União Europeia, assinaram o Plano de Ação Conjunto Global (na sigla em inglês, JPCOA). Por esse acordo, o Irã se comprometia a reduzir sua capacidade de enriquecer urânio. Em troca, ativos iranianos que foram retidos por causa de sanções da ONU referentes à política nuclear do país seriam liberados.\n",
    "O valor exato desses ativos é incerto. A estimativa mais alta é de que esses valores chegavam a US$ 150 bilhões. Entretanto, os valores estimados pelo próprio governo americano são bem menores. Segundo o Departamento do Tesouro dos Estados Unidos, o valor liberado é de “pouco mais de US$ 50 bilhões”. O acordo foi assinado durante a gestão de Barack Obama. Em 2018, Trump retirou a assinatura dos Estados Unidos.\n",
    "Assine a Lente, a newsletter gratuita sobre desinformação da Lupa!\n",
    "É importante pontuar duas coisas: primeiro, isso não significa que esse dinheiro foi dado ao Irã. Esses ativos sempre foram do Irã, mas estavam congelados em bancos estrangeiros por causa de sanções impostas pela Organização das Nações Unidas (ONU). Além disso, praticamente todo o dinheiro estava em bancos fora dos Estados Unidos. O Politifact classificou essa informação como parcialmente correta.\"\"\"\n",
    "\n",
    "def preprocessClaim(text):\n",
    "    text = text.replace(\"Assine a Lente, a newsletter gratuita sobre desinformação da Lupa!\", \"\")\n",
    "    text = re.sub(r'^“',\"\", text)\n",
    "    text = re.sub(r'”$',\"\", text)\n",
    "    text = text.replace(\"_OPENQUOTES_\",\"“\")\n",
    "    text = text.replace(\"_CLOSEQUOTES_\",\"”\")\n",
    "    return text.strip()\n",
    "\n",
    "def preprocessMetadata(text):\n",
    "    text = text.replace(\"Assine a Lente, a newsletter gratuita sobre desinformação da Lupa!\", \"\")\n",
    "    return text.strip()\n",
    "\n",
    "def preprocessEvidence(text):\n",
    "    text = text.replace(\"‌\",\"\")#Caractere estranho\n",
    "    text = text.replace(\"Assine a Lente, a newsletter gratuita sobre desinformação da Lupa!\", \"\")\n",
    "    text = re.sub(r'Nota:.*',\"\", text)\n",
    "    text = re.sub(r'Editado por:.*',\"\", text)\n",
    "    text = re.sub(r'Esta verificação foi sugerida por leitores.*',\"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "for item in items:\n",
    "    item['claim'] = preprocessClaim(item['claim'])\n",
    "    item['evidence'] = preprocessEvidence(item['evidence'])\n",
    "    item['metadata'] = preprocessMetadata(item['metadata'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'row', 'trecho', 'qtd_trechos', 'source', 'claim', 'metadata', 'class', 'evidence']\n"
     ]
    }
   ],
   "source": [
    "#Salvar\n",
    "with Path('bases/lupa_trechos_tratados.tsv').open('w', encoding=\"utf-8\", newline='') as f:\n",
    "    fieldnames = ['id', 'row', 'trecho', 'qtd_trechos', 'source', 'claim', 'metadata', 'class', 'evidence']\n",
    "    print(fieldnames)\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='\\t', extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for trecho in trechos:\n",
    "        #print(trecho['evidence'])\n",
    "        writer.writerow(trecho)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
