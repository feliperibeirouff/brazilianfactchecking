{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handy-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Buscar as urls das notícias e salvar no arquivo urls_lupa.tsv:\n",
    "\n",
    "import csv\n",
    "import math\n",
    "\n",
    "#Lista da quantidade de notícias de cada mês para cada ano. Usado para saber quantas páginas tem\n",
    "qtdPorAno = {\n",
    "    2021: [68, 63, 72, 65, 97, 41],\n",
    "    2020: [56, 50, 89, 169, 175, 181, 130, 109, 88, 168, 244, 70]\n",
    "}\n",
    "\n",
    "#Dado que uma página com a lista de notícias está aberta, retorna essa lista, com [url, data, vinheta, título, chamada] de cada notícia.\n",
    "def getLinks():\n",
    "    blocos = driver.find_elements_by_xpath('//div[@class=\"internaPGN\"]/div')\n",
    "    links = []\n",
    "    for bloco in blocos:\n",
    "        bloco_meta = bloco.find_element_by_class_name('bloco-meta').text\n",
    "        meta = bloco_meta.split('|')\n",
    "        date = meta[0].strip()\n",
    "        vinheta = meta[2].strip()    \n",
    "        bloco_chamada = bloco.find_element_by_xpath('.//h3[@class=\"bloco-chamada\"]/a')\n",
    "        url = bloco_chamada.get_attribute(\"href\")\n",
    "        title = bloco_chamada.get_attribute(\"title\")\n",
    "        chamada = bloco_chamada.text\n",
    "        links.append({'url': url, 'date': date, 'vinheta': vinheta, 'title': title, 'chamada' : chamada})\n",
    "    return links\n",
    "\n",
    "\n",
    "#Entra em cada página e salva a lista das notícias no arquivo\n",
    "with open('urls_lupa.tsv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['url', 'date', 'vinheta', 'title', 'chamada']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer.writeheader()    \n",
    "\n",
    "    for ano in qtdPorAno.keys():\n",
    "        qtdPorMes = qtdPorAno[ano]\n",
    "        for i, qtd in enumerate(qtdPorMes):\n",
    "            mes = str(i+1).zfill(2)    \n",
    "            print(ano, mes, qtd)\n",
    "            paginas = math.ceil(qtd/10)\n",
    "            for pagina in range(paginas):\n",
    "                page_group = 'https://piaui.folha.uol.com.br/lupa/'+str(ano)+'/'+mes+'/'+'page/'+str(pagina + 1)+'/'\n",
    "                driver.get(page_group)\n",
    "                links = getLinks()\n",
    "                for link in links:\n",
    "                    writer.writerow(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def saveLupaPage(filePath, claim):\n",
    "    \"\"\"! Acessa a página da notícia e salva a notícia completa\n",
    "    @param filePath - Caminho do arquivo que será salvo\n",
    "    @param claim - um dicionário com as informações principais da notícia (precisa ter os atributos url, date, vinheta, title, chamada).\n",
    "      Isso foi lido do arquivo urls_lupa.tsv\n",
    "    \"\"\"\n",
    "    url = claim['url']\n",
    "    driver.get(url)\n",
    "\n",
    "    with open(filePath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['row', 'url', 'date', 'vinheta', 'title', 'chamada', 'contentType','tags', 'content']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "        writer.writeheader()\n",
    "        try:\n",
    "            claim['contentType'] = driver.find_element_by_class_name('post-categ-label').text\n",
    "        except NoSuchElementException:\n",
    "            claim['contentType'] = 'NOTFOUND'\n",
    "        claim['content'] = driver.find_element_by_class_name('post-inner').text\n",
    "        elem_tags = driver.find_elements_by_xpath('//div[@class=\"post-tags no-print\"]/ul[2]/li')\n",
    "        tags = []\n",
    "        for tag in elem_tags:\n",
    "            tags.append(tag.text)\n",
    "        claim['tags'] = ';'.join(tags)\n",
    "        writer.writerow(claim)\n",
    "    return claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Lê a lista de urls de notícias, verifica se ela já foi salva, e se não foi, salva em um arquivo.\n",
    "# Cada notícia será salva num arquivo diferente, para facilitar a verificação de qual já foi salva.\n",
    "claims = [] \n",
    "path = 'lupa_files/'\n",
    "Path(path).mkdir(exist_ok=True) #parents=True, \n",
    "with open('urls_lupa.tsv', 'r', encoding='utf-8') as f:\n",
    "    read_urls = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "    for i, row in enumerate(read_urls):\n",
    "        print(i)\n",
    "        print(row)\n",
    "        row['row'] = i\n",
    "        url = row['url']\n",
    "        #Gera o nome do arquivo em fução da url\n",
    "        filePath = Path(path + url.replace(\"https://piaui.folha.uol.com.br/lupa/\", \"\").replace(\"/\", \"_\")+\".tsv\")\n",
    "        if not filePath.is_file():            \n",
    "            claims.append(saveLupaPage(filePath, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerar lista de arquivos ruins: spanish ou não checagem\n",
    "\n",
    "path = 'lupa_files/'\n",
    "for file in listdir(path):\n",
    "    full_path = join(path, file)\n",
    "    print(full_path)\n",
    "    with open(full_path, 'r', encoding='utf-8') as f:\n",
    "        read_claims = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "        for rclaims in read_claims:\n",
    "            print(rclaims['url'])\n",
    "            if rclaims['url'] not in bad_urls and isFactChecking(rclaims['content']):\n",
    "                claims.append(rclaims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Após salvar todas as páginas, essa parte verifica quais das páginas salvas realmente possuem alguma checagem de fatos, e junta elas num arquivo só (allLupaFiles.tsv)\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import csv\n",
    "\n",
    "\n",
    "def isFactChecking(text):\n",
    "    # Nem todas as páginas possuem checagem de fatos. Só possuem as que tiverem as seguintes classes\n",
    "    classes = ['VERDADEIRO, MAS', 'VERDADEIRO', 'EXAGERADO', 'FALSO', 'AINDA É CEDO PARA DIZER', 'CONTRADITÓRIO', 'SUBESTIMADO', 'INSUSTENTÁVEL', 'DE OLHO']\n",
    "    for classe in classes:\n",
    "        if classe in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def isSpanish(claim):\n",
    "    spanish_words = ['Es falso', 'Es falsa', ' lo ', ' la ']\n",
    "    for word in spanish_words:\n",
    "        if word in claim['title'] or word in claim['chamada']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# A lista de páginas que serão consideradas\n",
    "claims = []\n",
    "\n",
    "# Algumas páginas estão em espanhol, ou não precisam ser salvas. Coloquei as urls dessas notícias no arquivo lupa_ruins.txt para ignorá-las.\n",
    "bad_urls = []\n",
    "with open('bases/lupa_ruins.txt', 'r') as f:\n",
    "    for i in f:\n",
    "        bad_urls.append(i.strip())\n",
    "\n",
    "list_spanish = []\n",
    "list_not_factchecking = []\n",
    "\n",
    "path = 'bases/lupa_files/'\n",
    "for file in listdir(path):\n",
    "    full_path = join(path, file)\n",
    "    #print(full_path)\n",
    "    with open(full_path, 'r', encoding='utf-8') as f:\n",
    "        read_claims = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "        for rclaims in read_claims:\n",
    "            #print(rclaims['url'])\n",
    "            if not isFactChecking(rclaims['content']):\n",
    "                list_not_factchecking.append(rclaims['url'])\n",
    "            elif isSpanish(rclaims):\n",
    "                list_spanish.append(rclaims)\n",
    "            else:\n",
    "                if rclaims['url'] not in bad_urls:\n",
    "                    claims.append(rclaims)\n",
    "\n",
    "for i in list_spanish:\n",
    "    print('spanish:', i['url'], i['title'])\n",
    "\n",
    "print(len(claims))\n",
    "\n",
    "with open('bases/allLupaFiles.tsv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['row', 'url', 'date', 'vinheta', 'title', 'chamada', 'contentType','tags', 'content']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer.writeheader()\n",
    "    for i, claim in enumerate(claims):\n",
    "        claim['row'] = i\n",
    "        writer.writerow(claim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c624b7a7fae0c5d9b8099a97c898e13ad55c76ce55c8cdd808646eaeeda7047"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
