{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the pipeline it is necessary to have a file in claims_unprocessed_path, separated by tab, with the columns:<br>\n",
    "* id - Claim's unique numerical identifier<br>\n",
    "* claim - Text with the claim to be verified<br>\n",
    "* class - Optional. The expected class, in case you want to evaluate the result of the classifier.\n",
    "\n",
    "Each step can be run in a different execution as long as it follows the order. That is, if the Kernel needs to be restarted, it is not necessary to run the previous cells, except the cell that informs the paths of the files.\n",
    "\n",
    "Trained models must be placed in the models folder and can be obtained here:\n",
    "https://drive.google.com/drive/folders/1wZj_GJJ1O9goAPuYrTNahL2sE-jTk4Mm?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strategies import ClassifierStrategy, EvidenceSelectStrategy\n",
    "\n",
    "class FactCheckerParameters:\n",
    "    def __init__(self, document_search_method, evidence_selection_method, classifier_method, base_path, url_categories_path, claims_file):\n",
    "        self.document_search_method = document_search_method\n",
    "        self.evidence_selection_method = evidence_selection_method\n",
    "        self.classifier_method = classifier_method\n",
    "        self.base_path = base_path\n",
    "        self.url_categories_path = url_categories_path\n",
    "        self.claims_path = base_path  + claims_file\n",
    "        self.claims_unprocessed_path = self.claims_path.replace(\".tsv\", \"_raw.tsv\")\n",
    "        self.claims_sbert_path = self.claims_path.replace(\".tsv\", \"_sbert1.pt\")\n",
    "        self.evidences_path = self.claims_path.replace(\".tsv\", \"_ev\" + self.evInfo() + \".tsv\")\n",
    "        self.evidences_classified_path = self.evidences_path.replace('.tsv',  \"_\" + classifier_method.name + '_classified.tsv')\n",
    "        self.claims_classified_path = self.claims_path.replace('.tsv', self.classifierInfo() + '_classified.tsv')\n",
    "        self.documents_path = self.claims_path.replace(\".tsv\", self.docInfo() + \"_docret.tsv\")\n",
    "        self.urls_directory = self.getUrlsDir(self.documents_path)\n",
    "        self.claims_classified_same_doc_path = self.claims_path.replace('.tsv', self.classifierInfo() + '_classifiedSameDoc.tsv')\n",
    "        \n",
    "        print(\"base_path:\", self.base_path)\n",
    "        print(\"url_categories_path:\", self.url_categories_path)\n",
    "        print(\"claims_path:\", self.claims_path)\n",
    "        print(\"claims_unprocessed_path:\", self.claims_unprocessed_path)\n",
    "        print(\"claims_sbert_path:\", self.claims_sbert_path)\n",
    "        print(\"evidences_path:\", self.evidences_path)\n",
    "        print(\"evidences_classified_path:\", self.evidences_classified_path)\n",
    "        print(\"claims_classified_path:\", self.claims_classified_path)\n",
    "        print(\"documents_path:\", self.documents_path)\n",
    "        print(\"urls_directory:\", self.urls_directory)\n",
    "\n",
    "    def getUrlsDir(self, doc_path):\n",
    "        return doc_path.replace(\".tsv\", \"_urls/\")\n",
    "\n",
    "    def docInfo(self):\n",
    "        return \"_\" + self.document_search_method\n",
    "\n",
    "    def evInfo(self):\n",
    "        return self.docInfo() + \"_\" + self.evidence_selection_method.name\n",
    "\n",
    "    def classifierInfo(self):\n",
    "        return self.evInfo() + \"_\" + self.classifier_method.name\n",
    "\n",
    "params = FactCheckerParameters(\n",
    "    document_search_method = \"current\",\n",
    "    evidence_selection_method = EvidenceSelectStrategy.Context5,\n",
    "    classifier_method = ClassifierStrategy.Bert3,\n",
    "    base_path = \"bases/base3/\",\n",
    "    url_categories_path = \"urls_categories.txt\",\n",
    "    claims_file = \"base3_test.tsv\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_loader\n",
    "import preprocessing\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "claims = dataset_loader.loadClaims(params.claims_unprocessed_path)\n",
    "for claim in claims:\n",
    "  claim['claim_clean'] = preprocessing.tokenize_and_join(claim['claim'])\n",
    "\n",
    "with Path(params.claims_path).open('w', encoding=\"utf-8\", newline='') as f2:\n",
    "  fieldnames = ['id', 'claim_clean', 'class']\n",
    "  writer = csv.DictWriter(f2, fieldnames=fieldnames, delimiter='\\t', extrasaction='ignore')\n",
    "  writer.writeheader()\n",
    "  for claim in claims:\n",
    "    writer.writerow(claim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import document_retrieval\n",
    "import dataset_loader\n",
    "\n",
    "claims = dataset_loader.loadClaims(params.claims_path)\n",
    "document_retrieval.documentRetrieval(claims, params.documents_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import document_retrieval\n",
    "document_retrieval.extractDocuments(params.documents_path, params.urls_directory, params.url_categories_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving claims' embeddings\n",
    "from select_evidences import SbertEncoder\n",
    "import dataset_loader\n",
    "\n",
    "claims = dataset_loader.loadClaims(params.claims_path)\n",
    "encoder = SbertEncoder(params.urls_directory)\n",
    "encoder.save_sbert_claims(claims, params.claims_sbert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving documents' embeddings\n",
    "\n",
    "from select_evidences import SbertEncoder\n",
    "\n",
    "documents = dataset_loader.loadDocuments(params.documents_path, params.url_categories_path)\n",
    "encoder = SbertEncoder(params.urls_directory)\n",
    "encoder.save_sbert_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidence Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from select_evidences import get_evidence_selector\n",
    "from select_evidences import EvidenceSelectStrategy\n",
    "import dataset_loader\n",
    "\n",
    "claims = dataset_loader.loadClaims(params.claims_path)\n",
    "documents = dataset_loader.loadDocuments(params.documents_path, params.url_categories_path)\n",
    "get_evidence_selector(params.evidence_selection_method, params.urls_directory).selectEvidences(claims, documents, params.claims_sbert_path, params.evidences_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_loader\n",
    "import classification\n",
    "\n",
    "def classify(params):\n",
    "  claims = dataset_loader.loadClaims(params.claims_path)\n",
    "  print('Evidences:', params.evidences_path)\n",
    "  evidences = dataset_loader.loadEvidences(params.evidences_path)\n",
    "  \n",
    "  print(\"Classification:\", params.classifierInfo())\n",
    "  \n",
    "  classification.classify_claims(claims, evidences, params.evidences_classified_path, params.claims_classified_path, params.classifier_method)\n",
    "\n",
    "classify(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ia')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a5609854b1d1af0719aa43b8184aabedc7c5a84215f90de6c4b3f051bd446ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
