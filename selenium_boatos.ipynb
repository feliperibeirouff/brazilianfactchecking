{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inicializando driver do selenium\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "op = Options()\n",
    "op.set_preference('javascript.enabled', False)\n",
    "print(\"Starting selenium driver\")\n",
    "driver = webdriver.Firefox(options=op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "\n",
    "def extract_boato(url, category = None):\n",
    "  try:\n",
    "    print(\"get\", url)\n",
    "    #Go to the address\n",
    "    driver.get(url)\n",
    "    print(\"find published\")\n",
    "    #Extracting publication date\n",
    "    published = driver.find_element(By.CLASS_NAME, \"published\").text\n",
    "    print(\"published:\", published)\n",
    "\n",
    "    if not category:\n",
    "      category = driver.find_element(By.CLASS_NAME, \"cat-links\").text\n",
    "      print(\"category:\", category)\n",
    "\n",
    "    #Extracting title\n",
    "    title = driver.find_element(By.CLASS_NAME, \"entry-title\").text\n",
    "    print(\"Title:\", title)\n",
    "\n",
    "    #Extracting texts in red\n",
    "    red_elements = driver.find_elements(By.CLASS_NAME,\"red\")\n",
    "\n",
    "    if not red_elements:\n",
    "      red_elements = driver.find_elements_by_xpath('//span[@style=\"color: #ff0000;\"]')\n",
    "    \n",
    "    if not red_elements:\n",
    "      red_elements = driver.find_elements(By.TAG_NAME,\"blockquote\")\n",
    "      red_elements = [x for x in red_elements if not x.text.startswith('Confira tambÃ©m')]\n",
    "\n",
    "    if not red_elements:\n",
    "      red_elements = driver.find_elements(By.TAG_NAME,\"i\")\n",
    "      red_elements = [x for x in red_elements if x.text]\n",
    "\n",
    "    if not red_elements:\n",
    "      red_elements = driver.find_elements(By.TAG_NAME,\"em\")\n",
    "      red_elements = [x for x in red_elements if x.text]\n",
    "    \n",
    "    red_texts = []\n",
    "    for r in red_elements:\n",
    "      if r.text:\n",
    "        red_texts.append(r.text)\n",
    "\n",
    "    #Extracting paragraphs. They are tagged p\n",
    "    contents = driver.find_elements(By.TAG_NAME,\"p\")\n",
    "    content_texts = [content.text for content in contents if content.text != '']\n",
    "\n",
    "    #The first paragraph is a subtitle\n",
    "    subtitle = content_texts[0]\n",
    "    print(\"Subtitle:\", subtitle)\n",
    "\n",
    "    #Join paragraphs with a \\n and replace original news with the text \"_NOTICIA_ORIGINAL_\"\n",
    "    content = '\\n '.join(content_texts[1:])\n",
    "    for red_text in red_texts:\n",
    "      content = content.replace(red_text, '_NOTICIA_ORIGINAL_')\n",
    "      r = red_text.replace(\"\\n\",\"\\n \")\n",
    "      content = content.replace(r, '_NOTICIA_ORIGINAL_')\n",
    "\n",
    "    regexp = re.compile(r'_NOTICIA_ORIGINAL_[\\s\\n]*_NOTICIA_ORIGINAL_')\n",
    "    while regexp.search(content):\n",
    "      content = re.sub(r'_NOTICIA_ORIGINAL_[\\s\\n]*_NOTICIA_ORIGINAL_', '_NOTICIA_ORIGINAL_', content) # ; seguido de maiuscula, que provavelmente indica fim de frase\n",
    "    \n",
    "    blocos_noticia = content.count('_NOTICIA_ORIGINAL_')\n",
    "    if blocos_noticia > 1:\n",
    "      return {'url': url, 'status':'MAIS_DE_UM_VERMELHO', 'title':title, 'subtitle':'', 'original_news':'', 'content':'', 'published': published, 'category': category}\n",
    "    elif blocos_noticia == 0:\n",
    "      return {'url': url, 'status':'SEM_VERMELHO', 'title':title, 'subtitle':'', 'original_news':'', 'content':'', 'published': published, 'category': category}\n",
    "\n",
    "    return {'url': url,'title': title, 'subtitle': subtitle, 'original_news': '\\n'.join(red_texts), 'content': content, 'status':'OK', 'published': published, 'category': category}\n",
    "  except Exception as e:\n",
    "    return {'url': url, 'status':'ERRO', 'title':'', 'subtitle':'', 'original_news':'', 'content':str(e), 'published': '', 'category': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "x = 'bases/fakepedia_urls_without_duplicates.txt'\n",
    "urls = []\n",
    "with Path(x).open('r', encoding=\"utf-8\") as f:\n",
    "  read_urls = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "  for url in read_urls:\n",
    "    #print(url)\n",
    "    urls.append(url['url_review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting from boatos.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "documents_path = 'bases/fakepedia_boatos.tsv'\n",
    "\n",
    "last_id = ''\n",
    "try:\n",
    "    with Path(documents_path).open('r', encoding=\"utf-8\") as f:\n",
    "        read_documents = csv.DictReader(f, delimiter=\"\\t\", skipinitialspace=True)\n",
    "        for doc in read_documents:\n",
    "            last_id = doc['url']\n",
    "            #print(last_id)\n",
    "except:\n",
    "    pass\n",
    "print('last_id:', last_id)\n",
    "\n",
    "ignore_claims = (last_id != '')\n",
    "\n",
    "with Path(documents_path).open('a', encoding=\"utf-8\", newline='') as f:\n",
    "    fieldnames = ['url', 'title', 'subtitle', 'original_news', 'content', 'status', 'published', 'category', 'row']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='\\t', extrasaction='ignore')\n",
    "    if last_id == '':\n",
    "        writer.writeheader()\n",
    "    row = 0\n",
    "    for url in urls:\n",
    "        if ignore_claims or url == last_id:\n",
    "            if url == last_id:\n",
    "                ignore_claims = False\n",
    "            continue            \n",
    "        print(str(datetime.now()), row, url)\n",
    "        boato = extract_boato(url)\n",
    "        boato['row'] = row\n",
    "        row += 1\n",
    "        writer.writerow(boato)\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If necessary, it is possible to get URLs per page\n",
    "\n",
    "urls = []\n",
    "for i in range(1):\n",
    "  url = f\"https://www.boatos.org/category/politica/page/{i+1}\"\n",
    "  driver.get(url)\n",
    "  list_published = driver.find_elements(By.CLASS_NAME, \"entry-title\")\n",
    "  contents = driver.find_elements_by_css_selector(\".entry-title a\")\n",
    "  for link in contents:\n",
    "    urls.append(link.get_property(\"href\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a5609854b1d1af0719aa43b8184aabedc7c5a84215f90de6c4b3f051bd446ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
